# 통계학 4주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_4th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

4주차는 `2부-11.데이터 전처리와 파생변수 생성`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.

[실습코드](https://github.com/c-karl/DA_DS_Book001)를 참고하여 학습해주세요.


## Statistics_4th_TIL

### 2부. 데이터 분석 준비하기
### 11.데이터 전처리와 파생변수 생성



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|--------------|----------|
|1주차| 1부 ~p.79    | ✅      |
|2주차| 2부 ~p.120   | ✅      | 
|3주차| 2부 ~p.202   | ✅      | 
|4주차| 2부 ~p.299   | ✅      | 
|5주차| 3부 ~p.356   | 🍽️      | 
|6주차| 3부 ~p.437   | 🍽️      | 
|7주차| 3부 ~p.542   | 🍽️      | 
|8주차| 3부 ~p.615   | 🍽️      | 
|9주차|데이터 분석 실습| 🍽️      |

<!-- 여기까진 그대로 둬 주세요-->

# 11.데이터 전처리와 파생변수 생성

```
✅ 학습 목표 :
* 데이터 전처리 방법을 이해하고 적용할 수 있다.
* 데이터 변환과 가공 기법을 학습하고 활용할 수 있다.
* 모델 성능을 향상시키는 데이터 가공 기법을 이해하고 적용할 수 있다.
* 데이터의 유사도를 측정하는 다양한 거리 측정 방법을 이해하고 활용할 수 있다.
```
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

## 결측값 처리
### 결측치(missing value)의 종류
1. 완전 무작위 결측(MCAR: Missing Completely at Random): 순수하게 결측값이 무작위로 발생한 경우. 이 경우, 결측값을 포함한 데이터를 제거해도 편향(bias)이 거의 발생되지 않음
2. 무작위 결측(MAR: Missing at Random): 다른 변수의 특성에 의해 해당 변수의 결측치가 체계적으로 발생한 경우 (ex. A마트의 전국 체인 매출 정보 중, 특정 체인점의 POS기기에 오류가 나서 해당 체인점에 해당하는 매출 정보에 결측값이 많이 나타난 경우)
3. 비무작위 결측(NMAR: Missing at Not Random): 결측값들이 해당 변수 자체의 특성을 갖고 있는 경우 (ex. A마트의 고객정보 데이터에서 '고객 소득' 변수에서 결측값들 대부분이 소득이 적어서 소득을 공개하기 꺼려해서 결측이 발생한 경우)

### 결측치 처리 방법
1. 완전 무작위 결측일 경우
- 표본 제거 방법: 결측값이 심하게 많은 변수를 제거하거나, 결측값이 포함된 행을 제외하고 데이터 분석을 하는 방법. 전체 데이터에서 결측값 비율이 10% 미만일 경우, 이 방법을 많이 사용함
- 평균 대치법: 결측값을 제외한 온전한 값들의 평균을 구한 다음, 그 평균 값을 결측값들에 대치하는 것. 그러나 이로 인해 통계량의 표준오차가 왜곡되어 축소되어 나타나고, p-value가 부정확하게 됨
2. 데이터가 시계열적 특성일 경우
- 보간법(interpolation): 단순하게 평균값으로 대치
    - 단순 순서 보간법
    - 시점 고려 보간법
- 회귀 대치법(regression imputation): 해당 변수와 다른 변수 사이의 관계성을 고려하여 결측값을 계산 (ex. '연령' 변수의 결측값을 대치하기 위해 '연 수입' 변수를 사용하는 것). 그러나 결측된 변수의 분산을 과소 추정하는 문제가 있음
- 확률적 회귀대치법(stochastic regression imputation): 표준오차 과소 추정 문제를 해결하기 위해 인위적으로 회귀식에 확률 오차항을 추가하는 방식
- 대중 대치법(multiple imputation): 단순 대치를 여러 번 수행하여 n개의 가상적 데이터를 생상하여 이들의 평균으로 결측값을 대치하는 방법
    - 대치 단계
    - 분석 단계
    - 결합 단계


## 이상치 처리
- 이상치: 일부 관측치의 값이 전체 데이터의 범위에서 크게 벗어난 아주 작거나 큰 근단적인 값
### 이상치 처리 방법
1. 이상치 제거(trimming): 추정치의 분산은 감소하지만, 실젯값을 과장하여 편향을 발생시킴
2. 관측값 변경(value modification): 하한 값과 상한 값을 결정한 후 하한 값보다 작으면 하한 값으로 대체하고 상한 값보다 크면 상한 값으로 대체
3. 가중치 조정(weight modification): 이상치의 영향을 감소시키는 가중치를 부여
### 이상치 처리시 유의할 점
- 효과적인 이상치 탐색을 위해서는 해당 데이터 변수들의 의미와 비즈니스 도메인을 먼저 이해하고, 이상치가 생긴 원인을 논리적으로 생각하여 데이터를 바라봐야 함
- 분석 도메인에 따라 이상치가 중요한 분석 요인일 수 있음 (ex. 제품의 불량 원인을 찾아내기 위해 특정 공정의 센서 데이터 변화를 분석하는 경우 → 이상치가 분석의 중요한 요소가 됨)

![]()

## 변수 구간화(Binning)
- 변수 구간화(Binning): 데이터 분석의 성능을 향상시키기 위해서, 혹은 해석의 편리성을 위해 이산형 변수를 범주형 변수로 변환하는 것
- 목적: 이산형 변수를 범주형 변수로 비즈니스적 상황에 맞도록 변환시킴으로써 데이터의 해석이나 예측, 분류 모델을 의도에 맞도록 유도하는 것
### 변수 구간화 방법
1. 평활화(smoothing): 변수의 값을 일정한 폭이나 빈도로 구간을 나눈 후, 각 구간 안에 속한 데이터의 값을 평균, 중앙값, 경곗값 등으로 변환해주는 것
2. 클러스터링(머신러닝 기법1): 타깃 변수 설정이 필요 없이 구간화할 변수의 값들을 유사한 수준끼리 그룹화
3. 의사결정나무(머신러닝 기법2): 타깃 변수를 설정해, 구간화할 변수의 값을 타깃 변수 예측에 가장 적합한 구간으로 나누어줌

## 데이터 표준화와 정규화 스케일링
- 표준화, 정규화의 중요성: 표준화와 정규화는 거리를 활용한 군집 분석(k-Nearest Neighbor, 서포트 벡터 머신)에서 필수적임
1. 표준화(Z-score): 각 관측치의 값이 전체 평균을 기준으로 어느 정도 떨어져 있는지 나타낼 때 사용. 평균은 0으로 변환, 1 표준편차 거리는 ±1, 2 표준편차 거리는 ±2로 변환
2. 정규화: 데이터의 범위를 0부터 1까지로 변환하여 데이터 분포를 조정

![]()

![]()

- 그래프를 보면 기존에는 달라보이던 변수 X1과 X2가 변환 후 매우 유사한 모습을 보이는 것을 알 수 있음

3. RobustScaler: 이상치에 민감하다는 표준화, 정규화 방식을 보완한 스케일링 기법. 중앙값(Q2)를 0으로 잡고, Q1과 Q3 사분위수와의 IQR 차이를 1이 되도록 하는 스케일링 기법. 이상치의 영향력을 최소화하여 일반적으로 표준화, 정규화보다 성능이 우수함


## 파생 변수 생성
- 파생 변수: 원래 있던 변수들을 조합하거나 함수를 적용하여 새로 만들어낸 변수
- 파생 변수는 데이터의 특성을 이용하여 분석 효율을 높이는 것이기 때문에 전체 데이터에 대한 파악이 중요할 뿐만 아니라 해당 비즈니스 도메인에 대한 충분한 이해가 수반되어야 함

![]()

![]()

- 파생 변수 생성 시 유의할 점: 파생변수는 기존의 변수를 활용해서 만들어낸 변수이기 때문에 **다중공선성** 문제가 발생할 가능성이 높음. 따라서 파생 변수를 만든 다음에는 상관분석을 통해 변수 간의 상관성을 확인할 필요가 있음. 상관성에 따라 파생변수를 사용할 지 여부를 결정할 수 있음

## 슬라이딩 윈도우 데이터 가공
- **예측 모델**에서 주로 사용. 데이터를 겹쳐 나눔으로써 전체 데이터가 증가하는 원리를 차용하여, 더 많은 분석 데이터셋을 확보하고 학습데이터의 최근성을 가질 수 있음

## 범주형 변수의 가변수 처리
- 가변수(더미 변수, Dummy variable): 범주형 변수를 0과 1의 값을 가지는 변수로 변환해주는 것
- 범주형 변수는 사용할 수 없고 연속형 변수만 사용 가능한 분석기법(선형 회귀분석, 로지스틱 회귀분석 등)을 사용하기 위함

![]()

![]()

- 가변수 처리를 할 때는, 원래 범주의 개수보다 하나 적게 가변수를 만들어야 함(이때 제거된 범주는 baseline이라 하며, 일반적으로 종속변수에 대한 영향력이 가장 적은 범주를 제거함). 이는 변수 간의 독립성을 유지시키고, 다중공선성 문제를 예방하기 위해서 임


## 언더샘플링과 오버샘플링
- 0과 1의 이진분류 모델에서 1의 비율이 매우 적은 경우 → **클래스 불균형** 문제 발생, 우리 원하는대로 학습이 이루어지지 않음
### 데이터 불균형 문제 해결 방법
1. 가중치 밸런싱(Weight balancing)
- 중요도가 높은 클래스를 잘못 분류했을 때 더 큰 손실을 계산하도록 조정하는 것 (가령, 중요한 10% 비중의 클래스를 잘못 분류하면 90%의 손실 가중치를 주고, 상대적으로 덜 중요한 90% 비중의 가중치를 잘못 분류하면 10% 손실 가중치를 주도록 설정할 수 있음)
2. 언더샘플링/오버샘플링
- 불균형 데이터 자체를 균형이 맞도록 가공하는 방법
- 큰 비중의 클래스 데이터를 줄이는 언더샘플링과 작은 비중의 클래스 데이터를 늘리는 오버샘플링
- 이 방법을 통해 머신러닝 모델의 성능을 향상시킬 수 있음

![]()

## 데이터 거리 측정 방법
- 데이터 거리 측정 ≒ 데이터 유사도 측정
- 공간상 데이터들 간의 거리가 가까우면 유사하다고 볼 수 있는데, 2차원이나 3차원상 관측치와 달리, 4차원 이상의 다차원 관측치의 경우 시각적으로 표헌하기 어렵다는 문제가 있음. 그러나 거리 측정 공식을 사용하면 다차원 데이터 간의 거리 측정이 가능함
- 단, 데이터 거리를 측정하기 전에 **데이터 표준화/정규화** 작업이 필요함
### 대표적인 거리 측정 방법
1. 유클리드 거리: 관측치 간의 직선거리를 측정함
2. 맨해튼 거리
3. 민코프스키 거리
4. 체비쇼프 거리
5. 마할라노비스 거리
6. 코사인 거리


<br>
<br>

# 확인 문제

## 문제 1. 데이터 전처리

> **🧚 한 금융회사의 대출 데이터에서 `소득` 변수에 결측치가 포함되어 있다. 다음 중 가장 적절한 결측치 처리 방법은 무엇인가?   
1️⃣ 결측값이 포함된 행을 모두 제거한다.  
2️⃣ 결측값을 `소득` 변수의 평균값으로 대체한다.  
3️⃣ `연령`과 `직업군`을 독립변수로 사용하여 회귀 모델을 만들어 `소득` 값을 예측한다.  
4️⃣ 결측값을 보간법을 이용해 채운다.**

> **[데이터 특징]**     
    - `소득` 변수는 연속형 변수이다.  
    - 소득과 `연령`, `직업군` 간에 강한 상관관계가 있다.  
    - 데이터셋에서 `소득` 변수의 결측 비율은 15%이다.

<!--결측값이 무작위로 발생한 경우인지(MCAR, MAR, NMAR) 판단하고, 변수 간 관계를 고려해보세요.-->

```
3️⃣번
결측값이 비무작위적으로 발생했을 가능성(NMAR)이 있음
결측치가 10%를 넘고, 소득 변수가 연속형이며, 연령, 직업군 간의 강한 상관관계가 있으므로,
회귀 대치법을 사용하는 것이 가장 적절함
```

## 문제 2. 이상치 처리

> **🧚 한 부동산 데이터에서 `주택 가격` 변수의 분포를 살펴본 결과, 대부분의 가격이 2억~5억 원 사이에 분포하지만, 100억 원 이상인 데이터가 일부 존재했다.**

> **🔍 Q1. 이러한 `주택 가격` 데이터를 이상치로 판단할 수 있는 방법을 한 가지 이상 서술하세요.**

```
IQR을 이용하여, Q3 + 1.5IQR(혹은 Q1 -1.5IQR)의 범위에서 벗어나는 값을 이상치로 판단할 수 있음
```

> **🔍 Q2. 이상치를 처리하는 방법에 대해 고민해보고 어떤 방법이 가장 적절할지 서술해주세요.**

<!-- 정해진 답은 없습니다. 자유롭게 작성해주세요-->

```
이상치 처리는 해당 데이터 변수들의 의미와 비즈니스 도메인을 먼저 이해하여 적절한 방법을 선택해야 함

이상치 자체를 제거(trimming)하거나, 로그 변환을 해서 처리할 수 있을 것임
```

## 문제 3. 데이터 스케일링

> **🧚 머신러닝 모델을 학습하는 과정에서, `연봉(단위: 억 원)`과 `근속연수(단위: 개월)`을 동시에 독립변수로 사용해야 합니다. 연봉과 근속연수를 같은 스케일로 맞추기 위해 어떤 스케일링 기법을 적용하는 것이 더 적절한가요?**

<!--표준화와 정규화의 차이점에 대해 고민해보세요.-->

```
표준화(Z-score)가 적절할 것임

연봉과 근속연수는 모두 연속형 변수이지만,
서로 다른 단위를 사용하고 있으므로 평균을 0, 표준편차를 1로 설정하면 서로 비교하기 용이할 것임
```

## 문제 4. 파생변수 생성

> **🧚 한 온라인 쇼핑몰에서는 고객의 구매 패턴을 분석하기 위해 기존 데이터에 파생변수를 추가하려고 한다.**

> 원본 데이터의 컬럼명: `총구매금액`, `방문횟수`, `최근 방문일`, `회원가입일`

> **🔍 Q1. 원본 데이터의 변수를 참고하여 의미 있는 파생 변수의 예시를 하나 이상 들어주세요.**

<!-- 정해진 답은 없습니다. 자유롭게 작성해주세요-->

```
방문횟수 당 구매금액 = 총구매금액 / 방문횟수
```

> **🔍 Q2. 파생변수를 만들 때 주의해야 할 점을 설명하세요.**

```
파생변수는 기존의 변수를 활용해서 만들어낸 변수이기 때문에 다중공선성 문제가 발생할 가능성이 높음
따라서 파생 변수를 만든 다음에는 상관분석을 통해 변수 간의 상관성을 확인할 필요가 있음
상관성에 따라 파생변수를 사용할 지 여부를 결정할 수 있음
```


## 문제 5. 클래스 불균형 해결

> **🧚 한 보험사에서 고객의 `사기 보험 청구 여부(0/1)`를 예측하는 분류 모델을 만들었다. 하지만 사기 청구 비율은 전체 데이터의 2%에 불과하다.**

> **🔍 Q1. 데이터 불균형 문제가 발생하면 모델 성능에 어떤 영향을 미칠 수 있나요?**

<!--사기 탐지 모델에서 발생할 데이터 불균형으로 인한 모델 성능에의 영향에 대해 구체적으로 고민해주세요.-->

```
클래스 불균형 문제 발생하여, 소수 클래스가 제대로 분류되지 않더라도 전체적인 분류 정확도가 높은 방향으로 학습이 됨

즉, 모델의 정확도가 떨어짐
```

> **🔍 Q2. 이 문제에서 적절한 해결 방법을 고민하여 서술해주세요.**

<!-- 정해진 답은 없습니다. 자유롭게 작성해주세요-->

```
사기 청구 비율이 2%에 불과하므로,
사기 청구 데이터에 더 높은 가중치를 부여하는 '가중치 밸런싱'을 통해 해결할 수 있음
```

## 문제 6. 데이터 거리 측정

> **🧚 한 추천 시스템에서 고객 간 유사도를 측정하려고 합니다. 고객의 `구매 내역`을 벡터로 변환한 후, 두 고객이 얼마나 유사한지를 측정하려면 유클리드 거리와 코사인 거리 중 어떤 거리 측정 방법이 더 적절한가요?**

<!--유클리드 거리는 벡터의 크기(절대적 차이)를 고려하고, 코사인 유사도는 방향(패턴)을 고려합니다.-->

```
코사인 거리
```


### 🎉 수고하셨습니다.